{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86cc17b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy.random' has no attribute 'BitGenerator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/kaixin/COVID_Mobility/Data/scripts/crawl_baidu_index/baidu-qianxi-crawler/simon-baidu-qianxi-crawler.ipynb Cell 1\u001b[0m line \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B81.70.197.155/home/kaixin/COVID_Mobility/Data/scripts/crawl_baidu_index/baidu-qianxi-crawler/simon-baidu-qianxi-crawler.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B81.70.197.155/home/kaixin/COVID_Mobility/Data/scripts/crawl_baidu_index/baidu-qianxi-crawler/simon-baidu-qianxi-crawler.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/__init__.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[39m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m is_numpy_dev \u001b[39mas\u001b[39;00m _is_numpy_dev  \u001b[39m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m \u001b[39mimport\u001b[39;00m hashtable \u001b[39mas\u001b[39;00m _hashtable, lib \u001b[39mas\u001b[39;00m _lib, tslib \u001b[39mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/compat/__init__.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplatform\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_typing\u001b[39;00m \u001b[39mimport\u001b[39;00m F\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_constants\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     IS64,\n\u001b[1;32m     19\u001b[0m     PY39,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     PYPY,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompressors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_typing.py:139\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    132\u001b[0m Frequency \u001b[39m=\u001b[39m Union[\u001b[39mstr\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBaseOffset\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    133\u001b[0m Axes \u001b[39m=\u001b[39m Union[AnyArrayLike, List, \u001b[39mrange\u001b[39m]\n\u001b[1;32m    135\u001b[0m RandomState \u001b[39m=\u001b[39m Union[\n\u001b[1;32m    136\u001b[0m     \u001b[39mint\u001b[39m,\n\u001b[1;32m    137\u001b[0m     ArrayLike,\n\u001b[1;32m    138\u001b[0m     np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mGenerator,\n\u001b[0;32m--> 139\u001b[0m     np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mBitGenerator,\n\u001b[1;32m    140\u001b[0m     np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mRandomState,\n\u001b[1;32m    141\u001b[0m ]\n\u001b[1;32m    143\u001b[0m \u001b[39m# dtypes\u001b[39;00m\n\u001b[1;32m    144\u001b[0m NpDtype \u001b[39m=\u001b[39m Union[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39mdtype, type_t[Union[\u001b[39mstr\u001b[39m, \u001b[39mcomplex\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mobject\u001b[39m]]]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy.random' has no attribute 'BitGenerator'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# import pandas as pd\n",
    "# import urllib\n",
    "# import ast\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import datetime\n",
    "# from calendar import monthrange\n",
    "\n",
    "# code_data = pd.read_csv(\"code_data.csv\") #data associating cities and regions names to their respective numeric codes\n",
    "\n",
    "# month = datetime.datetime.now().month #take current month\n",
    "# year = datetime.datetime.now().year #take current year\n",
    "# numdays = monthrange(year, month)[1] #number of days in current month\n",
    "# dates_sep = [str(datetime.date(year, month, day)) for day in range(1, numdays+1)] #dates as hyphen separated strings for naming\n",
    "# dates = [date.replace(\"-\", \"\") for date in dates_sep] #dates as solid strings for algorithm readability\n",
    "\n",
    "# code_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86ee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]\n",
      "  0%|                                                                                          | 0/369 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▏                                                                                 | 1/369 [00:01<06:13,  1.02s/it]\u001b[A\n",
      "  1%|▍                                                                                 | 2/369 [00:02<06:17,  1.03s/it]\u001b[A\n",
      "  1%|▋                                                                                 | 3/369 [00:02<06:01,  1.01it/s]\u001b[A\n",
      "  1%|▉                                                                                 | 4/369 [00:03<06:02,  1.01it/s]\u001b[A\n",
      "  1%|█                                                                                 | 5/369 [00:05<07:25,  1.22s/it]\u001b[A\n",
      "  2%|█▎                                                                                | 6/369 [00:06<06:57,  1.15s/it]\u001b[A\n",
      "  2%|█▌                                                                                | 7/369 [00:07<06:35,  1.09s/it]\u001b[A\n",
      "  2%|█▊                                                                                | 8/369 [00:08<06:18,  1.05s/it]\u001b[A\n",
      "  2%|██                                                                                | 9/369 [00:09<06:02,  1.01s/it]\u001b[A\n",
      "  3%|██▏                                                                              | 10/369 [00:11<07:11,  1.20s/it]\u001b[A\n",
      "  3%|██▍                                                                              | 11/369 [00:12<06:40,  1.12s/it]\u001b[A\n",
      "  3%|██▋                                                                              | 12/369 [00:12<06:19,  1.06s/it]\u001b[A\n",
      "  4%|██▊                                                                              | 13/369 [00:13<06:03,  1.02s/it]\u001b[A\n",
      "  4%|███                                                                              | 14/369 [00:14<06:01,  1.02s/it]\u001b[A\n",
      "  4%|███▎                                                                             | 15/369 [00:15<05:51,  1.01it/s]\u001b[A\n",
      "  4%|███▌                                                                             | 16/369 [00:16<05:46,  1.02it/s]\u001b[A\n",
      "  5%|███▋                                                                             | 17/369 [00:18<06:08,  1.05s/it]\u001b[A\n",
      "  5%|███▉                                                                             | 18/369 [00:18<05:55,  1.01s/it]\u001b[A\n",
      "  5%|████▏                                                                            | 19/369 [00:19<05:57,  1.02s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "### Extract move-in data from Baidu qianxi ###\n",
    "error_code = [] #error produced by missing data\n",
    "error_date = [] #date associated to error\n",
    "errors = []\n",
    "moveins = [] #list for move-in data\n",
    "\n",
    "for d in tqdm(range(len(dates))): #loop over dates\n",
    "    date = dates[d]\n",
    "    by_cities = [] #list for data from cities at a given date\n",
    "    for i in tqdm(range(len(code_data))):\n",
    "        code = code_data.City_Code[i] #take code of city for matching\n",
    "        #qianxi website by code and date\n",
    "        url = \"http://huiyan.baidu.com/migration/cityrank.jsonp?dt=province&id=\"+str(code)+\"&type=move_in&date=\"+date\n",
    "        #try runing code or except if error (add to error lists)\n",
    "        try:\n",
    "            file = urllib.request.urlopen(url, timeout=20) #increase timeout to avoid connection error\n",
    "            file = file.read() \n",
    "            dict_str = file.decode(\"UTF-8\") #decore file from URL as UTF-8 chracter map\n",
    "            dict_str = dict_str.replace('\\ncb({\"errno\":0,\"errmsg\":\"SUCCESS\",\"data\":{\"list\":[{', \"{\") #transform to string\n",
    "            dict_str = dict_str.replace(\"]}})\", \"\") #remove irrelevant characters\n",
    "            data = ast.literal_eval(dict_str) #turn string into data\n",
    "            data = pd.DataFrame(list(data)) #create dataframe\n",
    "            # add column names and data to dataframe by matching city codes\n",
    "            data.columns = [\"City_CH\", \"Prov_CH\", \"proportion\"]\n",
    "            data[\"City_EN_origin\"] = [code_data[code_data.City_CH==c].City_EN.values[0] for c in data.City_CH]\n",
    "            data[\"Prov_EN_origin\"] = [code_data[code_data.City_CH==c].Prov_EN.values[0] for c in data.City_CH]\n",
    "            data[\"City_EN_destination\"] = np.repeat(code_data[code_data.City_Code==code].City_EN.values[0], len(data))\n",
    "            data[\"Prov_EN_destination\"] = np.repeat(code_data[code_data.City_Code==code].Prov_EN.values[0], len(data))\n",
    "            data[\"City_CH_origin\"] = [code_data[code_data.City_CH==c].City_CH.values[0] for c in data.City_CH]\n",
    "            data[\"Prov_CH_origin\"] = [code_data[code_data.City_CH==c].Prov_CH.values[0] for c in data.City_CH]\n",
    "            data[\"City_CH_destination\"] = data.City_CH.values\n",
    "            data[\"Prov_CH_destination\"] = data.Prov_CH.values\n",
    "            data[\"City_code_origin\"] = [code_data[code_data.City_CH==c].City_Code.values[0] for c in data.City_CH]\n",
    "            data[\"Prov_code_origin\"] = [code_data[code_data.City_CH==c].Prov_Code.values[0] for c in data.City_CH]\n",
    "            data[\"City_code_destination\"] = np.repeat(code_data[code_data.City_Code==code].City_Code.values[0], len(data))\n",
    "            data[\"Prov_code_destination\"] = np.repeat(code_data[code_data.City_Code==code].Prov_Code.values[0], len(data))\n",
    "            by_cities.append(data)\n",
    "        except:\n",
    "            error_code.append(code)\n",
    "            error_date.append(dates_sep[d])\n",
    "            continue\n",
    "    errors.append(pd.DataFrame({\"error_code\":error_code, \"error_date\":error_date})) #errors list\n",
    "    data = pd.concat(by_cities) #data by city at a given date\n",
    "    data['date'] = dates_sep[d] #add dates to dataframe\n",
    "    moveins.append(data) #data list\n",
    "#errors = = pd.concat(errors)\n",
    "#errors.to_csv(\"missing_movein.csv\") #save errors dataframe (optional)\n",
    "data = pd.concat(moveins)\n",
    "data.to_csv(\"baidu_qianxi_data_movein\"+year+month+\".csv\", index=False, encoding='utf_8_sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]\n",
      "  0%|                                                                                          | 0/369 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▏                                                                                 | 1/369 [00:01<06:23,  1.04s/it]\u001b[A\n",
      "  1%|▍                                                                                 | 2/369 [00:02<06:17,  1.03s/it]\u001b[A\n",
      "  1%|▋                                                                                 | 3/369 [00:03<06:01,  1.01it/s]\u001b[A\n",
      "  1%|▉                                                                                 | 4/369 [00:03<05:55,  1.03it/s]\u001b[A\n",
      "  1%|█                                                                                 | 5/369 [00:04<05:52,  1.03it/s]\u001b[A\n",
      "  2%|█▎                                                                                | 6/369 [00:05<05:59,  1.01it/s]\u001b[A\n",
      "  2%|█▌                                                                                | 7/369 [00:07<06:08,  1.02s/it]\u001b[A\n",
      "  2%|█▊                                                                                | 8/369 [00:08<06:14,  1.04s/it]\u001b[A\n",
      "  2%|██                                                                                | 9/369 [00:09<06:03,  1.01s/it]\u001b[A\n",
      "  3%|██▏                                                                              | 10/369 [00:10<05:56,  1.01it/s]\u001b[A\n",
      "  3%|██▍                                                                              | 11/369 [00:11<05:56,  1.00it/s]\u001b[A\n",
      "  3%|██▋                                                                              | 12/369 [00:12<06:17,  1.06s/it]\u001b[A\n",
      "  4%|██▊                                                                              | 13/369 [00:13<06:00,  1.01s/it]\u001b[A\n",
      "  4%|███                                                                              | 14/369 [00:14<05:52,  1.01it/s]\u001b[A\n",
      "  4%|███▎                                                                             | 15/369 [00:15<06:08,  1.04s/it]\u001b[A\n",
      "  4%|███▌                                                                             | 16/369 [00:16<06:01,  1.02s/it]\u001b[A\n",
      "  5%|███▋                                                                             | 17/369 [00:17<05:56,  1.01s/it]\u001b[A\n",
      "  5%|███▉                                                                             | 18/369 [00:18<05:49,  1.00it/s]\u001b[A\n",
      "  5%|████▏                                                                            | 19/369 [00:19<06:04,  1.04s/it]\u001b[A\n",
      "  5%|████▍                                                                            | 20/369 [00:20<05:56,  1.02s/it]\u001b[A\n",
      "  6%|████▌                                                                            | 21/369 [00:21<05:51,  1.01s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "### Extract move-out data from Baidu qianxi ###\n",
    "# operation is same as for move-in data above\n",
    "error_code = []\n",
    "error_date = []\n",
    "errors = []\n",
    "moveouts = []\n",
    "for d in tqdm(range(len(dates))):  \n",
    "    date = dates[d]\n",
    "    by_cities = []\n",
    "    for i in tqdm(range(len(code_data))):\n",
    "        code = code_data.City_Code[i]\n",
    "        url = \"http://huiyan.baidu.com/migration/cityrank.jsonp?dt=province&id=\"+str(code)+\"&type=move_out&date=\"+date\n",
    "        try:\n",
    "            file = urllib.request.urlopen(url, timeout=20) #increase timeout to avoid connection error\n",
    "            file = file.read()\n",
    "            dict_str = file.decode(\"UTF-8\")\n",
    "            dict_str = dict_str.replace('\\ncb({\"errno\":0,\"errmsg\":\"SUCCESS\",\"data\":{\"list\":[{', \"{\")\n",
    "            dict_str = dict_str.replace(\"]}})\", \"\")\n",
    "            data = ast.literal_eval(dict_str)\n",
    "            data = pd.DataFrame(list(data))\n",
    "            data.columns = [\"City_CH\", \"Prov_CH\", \"proportion\"]\n",
    "            data[\"City_EN_destination\"] = [code_data[code_data.City_CH==c].City_EN.values[0] for c in data.City_CH]\n",
    "            data[\"Prov_EN_destination\"] = [code_data[code_data.City_CH==c].Prov_EN.values[0] for c in data.City_CH]\n",
    "            data[\"City_EN_origin\"] = np.repeat(code_data[code_data.City_Code==code].City_EN.values[0], len(data))\n",
    "            data[\"Prov_EN_origin\"] = np.repeat(code_data[code_data.City_Code==code].Prov_EN.values[0], len(data))\n",
    "            data[\"City_CH_destination\"] = [code_data[code_data.City_CH==c].City_CH.values[0] for c in data.City_CH]\n",
    "            data[\"Prov_CH_destination\"] = [code_data[code_data.City_CH==c].Prov_CH.values[0] for c in data.City_CH]\n",
    "            data[\"City_CH_origin\"] = data.City_CH.values\n",
    "            data[\"Prov_CH_origin\"] = data.Prov_CH.values\n",
    "            data[\"City_code_origin\"] = [code_data[code_data.City_CH==c].City_Code.values[0] for c in data.City_CH]\n",
    "            data[\"Prov_code_origin\"] = [code_data[code_data.City_CH==c].Prov_Code.values[0] for c in data.City_CH]\n",
    "            data[\"City_code_destination\"] = np.repeat(code_data[code_data.City_Code==code].City_Code.values[0], len(data))\n",
    "            data[\"Prov_code_destination\"] = np.repeat(code_data[code_data.City_Code==code].Prov_Code.values[0], len(data))\n",
    "            by_cities.append(data)\n",
    "        except:\n",
    "            error_code.append(code)\n",
    "            error_date.append(dates_sep[d])\n",
    "            continue\n",
    "    errors.append(pd.DataFrame({\"error_code\":error_code, \"error_date\":error_date}))\n",
    "    data = pd.concat(by_cities)\n",
    "    data['date'] = dates_sep[d]\n",
    "    moveouts.append(data)\n",
    "#errors = = pd.concat(errors)\n",
    "#errors.to_csv(\"missing_moveout.csv\")\n",
    "data = pd.concat(moveouts)\n",
    "data.to_csv(\"baidu_qianxi_data_moveout\"+year+month+\".csv\", index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b503a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
